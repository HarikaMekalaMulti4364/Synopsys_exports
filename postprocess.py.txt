# import json
# import collections
# from transformers import BertTokenizer

# class SquadPreprocessor:
#     def __init__(self, vocab_file='bert-base-uncased'):
#         self.tokenizer = BertTokenizer.from_pretrained(vocab_file)

#     def preprocess(self, input_file, max_seq_length=384, doc_stride=128, max_query_length=64):
#         with open(input_file, 'r') as f:
#             input_data = json.load(f)['data']

#         eval_examples = []
#         eval_features = []
#         input_ids_list = []
#         input_mask_list = []
#         segment_ids_list = []

#         for entry in input_data:
#             for paragraph in entry['paragraphs']:
#                 context_text = paragraph['context']
#                 for qa in paragraph['qas']:
#                     question_text = qa['question']
#                     example, features, input_ids, input_mask, segment_ids = self.convert_example_to_features(
#                         question_text, context_text, max_seq_length, doc_stride, max_query_length)
#                     eval_examples.append(example)
#                     eval_features.extend(features)  # features is a list of feature dictionaries
#                     input_ids_list.append(input_ids)
#                     input_mask_list.append(input_mask)
#                     segment_ids_list.append(segment_ids)

#         return input_ids_list, input_mask_list, segment_ids_list, eval_examples, eval_features

#     def convert_example_to_features(self, question, context, max_seq_length, doc_stride, max_query_length):
#         examples = {
#             "question": question,
#             "context": context
#         }
#         features = []

#         tok_to_orig_index = []
#         orig_to_tok_index = []
#         all_doc_tokens = []
#         for (i, token) in enumerate(context.split()):
#             orig_to_tok_index.append(len(all_doc_tokens))
#             sub_tokens = self.tokenizer.tokenize(token)
#             for sub_token in sub_tokens:
#                 tok_to_orig_index.append(i)
#                 all_doc_tokens.append(sub_token)

#         truncated_query = self.tokenizer.encode(
#             question, add_special_tokens=False, max_length=max_query_length, truncation=True)

#         max_tokens_for_doc = max_seq_length - len(truncated_query) - 3

#         _DocSpan = collections.namedtuple("DocSpan", ["start", "length"])
#         doc_spans = []
#         start_offset = 0
#         while start_offset < len(all_doc_tokens):
#             length = len(all_doc_tokens) - start_offset
#             if length > max_tokens_for_doc:
#                 length = max_tokens_for_doc
#             doc_spans.append(_DocSpan(start=start_offset, length=length))
#             if start_offset + length == len(all_doc_tokens):
#                 break
#             start_offset += min(length, doc_stride)

#         for (doc_span_index, doc_span) in enumerate(doc_spans):
#             tokens = []
#             token_to_orig_map = {}
#             token_is_max_context = {}
#             segment_ids = []
#             tokens.append("[CLS]")
#             segment_ids.append(0)
#             for token in truncated_query:
#                 tokens.append(token)
#                 segment_ids.append(0)
#             tokens.append("[SEP]")
#             segment_ids.append(0)

#             for i in range(doc_span.length):
#                 split_token_index = doc_span.start + i
#                 token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]

#                 is_max_context = self._check_is_max_context(doc_spans, doc_span_index, split_token_index)
#                 token_is_max_context[len(tokens)] = is_max_context
#                 tokens.append(all_doc_tokens[split_token_index])
#                 segment_ids.append(1)
#             tokens.append("[SEP]")
#             segment_ids.append(1)

#             input_ids = self.tokenizer.convert_tokens_to_ids(tokens)
#             input_mask = [1] * len(input_ids)

#             padding = [0] * (max_seq_length - len(input_ids))
#             input_ids += padding
#             input_mask += padding
#             segment_ids += padding

#             assert len(input_ids) == max_seq_length
#             assert len(input_mask) == max_seq_length
#             assert len(segment_ids) == max_seq_length

#             feature = {
#                 "input_ids": input_ids,
#                 "input_mask": input_mask,
#                 "segment_ids": segment_ids,
#                 "token_to_orig_map": token_to_orig_map,
#                 "token_is_max_context": token_is_max_context,
#                 "tokens": tokens
#             }
#             features.append(feature)
#         # print(input_ids)
#         # print(input_mask)
#         # exit()
#         return examples, features, input_ids, input_mask, segment_ids

#     def _check_is_max_context(self, doc_spans, cur_span_index, position):
#         best_score = None
#         best_span_index = None
#         for (span_index, doc_span) in enumerate(doc_spans):
#             end = doc_span.start + doc_span.length - 1
#             if position < doc_span.start:
#                 continue
#             if position > end:
#                 continue
#             num_left_context = position - doc_span.start
#             num_right_context = end - position
#             score = min(num_left_context, num_right_context) + 0.01 * doc_span.length
#             if best_score is None or score > best_score:
#                 best_score = score
#                 best_span_index = span_index



# input_file = "/media/ava/DATA3/DATA/Harika/inference/language/bert/data/dev-v1.1.json"
# output_file = 'preprocessed_data.json'
# max_seq_length = 384
# doc_stride = 128
# max_query_length = 64

# # with open(input_file, 'r') as f:
# #     input_data = json.load(f)

# preprocessor = SquadPreprocessor()
# input_ids, input_mask, segment_ids, eval_examples, eval_features = preprocessor.preprocess(
#     input_file, max_seq_length, doc_stride, max_query_length)

# output_data = {
#     "input_ids": input_ids,
#     "input_mask": input_mask,
#     "segment_ids": segment_ids,
#     "eval_examples": eval_examples,
#     "eval_features": eval_features
# }
# # print(output_data)
# max_input = 1  # Specify the maximum number of inputs you want to include

# # Assuming output_data is already populated
# output_dict_subset = {
#     "input_ids": output_data["input_ids"][:max_input],
#     "input_mask": output_data["input_mask"][:max_input],
#     "segment_ids": output_data["segment_ids"][:max_input],
#     "eval_examples": output_data["eval_examples"][:max_input],
#     "eval_features": output_data["eval_features"][:max_input]
# }

# with open(output_file, 'w') as f:
#     json.dump(output_data, f, indent=2)

# print(f"Preprocessing complete. Preprocessed data saved to {output_file}")

# import torch

# # Assuming you have output_data containing input_ids, input_mask, segment_ids
# input_ids = torch.tensor(output_data["input_ids"], dtype=torch.long)
# input_mask = torch.tensor(output_data["input_mask"], dtype=torch.long)
# segment_ids = torch.tensor(output_data["segment_ids"], dtype=torch.long)

# # Assuming model_input_ids, model_input_mask, model_segment_ids are the input names expected by your ONNX model
# onnx_input = {
#     "input_ids": input_ids,
#     "input_mask": input_mask,
#     "segment_ids": segment_ids
# }

# # Now you can send onnx_input to your ONNX model for inference
# # For example, if using ONNX Runtime:
# import onnxruntime as ort
# model_path = "/media/ava/DATA3/DATA/Harika/inference/language/bert/build/data/bert_tf_v1_1_large_fp32_384_v2/model.onnx"
# # Assuming model_path is the path to your ONNX model file
# session = ort.InferenceSession(model_path)
# output = session.run(None, onnx_input)
# print(output)

# exit()

# with open(output_file, 'w') as f:
#     json.dump(output_data, f, indent=2)

# print(f"Preprocessing complete. Preprocessed data saved to {output_file}")



import os
import json
import collections
from transformers import BertTokenizer

class SquadPreprocessor:
    def __init__(self, vocab_file='bert-base-uncased'):
        self.tokenizer = BertTokenizer.from_pretrained(vocab_file)

    def preprocess(self, input_file, max_seq_length=384, doc_stride=128, max_query_length=64):
        output_file = 'preprocessed_data.json'

        if os.path.exists(output_file):
            with open(output_file, 'r') as f:
                output_data = json.load(f)
            input_ids_list = output_data["input_ids"]
            input_mask_list = output_data["input_mask"]
            segment_ids_list = output_data["segment_ids"]
            eval_examples = output_data["eval_examples"]
            eval_features = output_data["eval_features"]
        else:
            with open(input_file, 'r') as f:
                input_data = json.load(f)['data']

            eval_examples = []
            eval_features = []
            input_ids_list = []
            input_mask_list = []
            segment_ids_list = []

            for entry in input_data:
                for paragraph in entry['paragraphs']:
                    context_text = paragraph['context']
                    for qa in paragraph['qas']:
                        question_text = qa['question']
                        example, features, input_ids, input_mask, segment_ids = self.convert_example_to_features(
                            question_text, context_text, max_seq_length, doc_stride, max_query_length)
                        eval_examples.append(example)
                        eval_features.extend(features)  # features is a list of feature dictionaries
                        input_ids_list.append(input_ids)
                        input_mask_list.append(input_mask)
                        segment_ids_list.append(segment_ids)

            output_data = {
                "input_ids": input_ids_list,
                "input_mask": input_mask_list,
                "segment_ids": segment_ids_list,
                "eval_examples": eval_examples,
                "eval_features": eval_features
            }

            with open(output_file, 'w') as f:
                json.dump(output_data, f, indent=2)

        return input_ids_list, input_mask_list, segment_ids_list, eval_examples, eval_features

    def convert_example_to_features(self, question, context, max_seq_length, doc_stride, max_query_length):
        examples = {
            "question": question,
            "context": context
        }
        features = []

        tok_to_orig_index = []
        orig_to_tok_index = []
        all_doc_tokens = []
        for (i, token) in enumerate(context.split()):
            orig_to_tok_index.append(len(all_doc_tokens))
            sub_tokens = self.tokenizer.tokenize(token)
            for sub_token in sub_tokens:
                tok_to_orig_index.append(i)
                all_doc_tokens.append(sub_token)

        truncated_query = self.tokenizer.encode(
            question, add_special_tokens=False, max_length=max_query_length, truncation=True)

        max_tokens_for_doc = max_seq_length - len(truncated_query) - 3

        _DocSpan = collections.namedtuple("DocSpan", ["start", "length"])
        doc_spans = []
        start_offset = 0
        while start_offset < len(all_doc_tokens):
            length = len(all_doc_tokens) - start_offset
            if length > max_tokens_for_doc:
                length = max_tokens_for_doc
            doc_spans.append(_DocSpan(start=start_offset, length=length))
            if start_offset + length == len(all_doc_tokens):
                break
            start_offset += min(length, doc_stride)

        for (doc_span_index, doc_span) in enumerate(doc_spans):
            tokens = []
            token_to_orig_map = {}
            token_is_max_context = {}
            segment_ids = []
            tokens.append("[CLS]")
            segment_ids.append(0)
            for token in truncated_query:
                tokens.append(token)
                segment_ids.append(0)
            tokens.append("[SEP]")
            segment_ids.append(0)

            for i in range(doc_span.length):
                split_token_index = doc_span.start + i
                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]

                is_max_context = self._check_is_max_context(doc_spans, doc_span_index, split_token_index)
                token_is_max_context[len(tokens)] = is_max_context
                tokens.append(all_doc_tokens[split_token_index])
                segment_ids.append(1)
            tokens.append("[SEP]")
            segment_ids.append(1)

            input_ids = self.tokenizer.convert_tokens_to_ids(tokens)
            input_mask = [1] * len(input_ids)

            padding = [0] * (max_seq_length - len(input_ids))
            input_ids += padding
            input_mask += padding
            segment_ids += padding

            assert len(input_ids) == max_seq_length
            assert len(input_mask) == max_seq_length
            assert len(segment_ids) == max_seq_length

            feature = {
                "input_ids": input_ids,
                "input_mask": input_mask,
                "segment_ids": segment_ids,
                "token_to_orig_map": token_to_orig_map,
                "token_is_max_context": token_is_max_context,
                "tokens": tokens
            }
            features.append(feature)

        return examples, features, input_ids, input_mask, segment_ids

    def _check_is_max_context(self, doc_spans, cur_span_index, position):
        best_score = None
        best_span_index = None
        for (span_index, doc_span) in enumerate(doc_spans):
            end = doc_span.start + doc_span.length - 1
            if position < doc_span.start:
                continue
            if position > end:
                continue
            num_left_context = position - doc_span.start
            num_right_context = end - position
            score = min(num_left_context, num_right_context) + 0.01 * doc_span.length
            if best_score is None or score > best_score:
                best_score = score
                best_span_index = span_index

# Usage example
input_file = "/media/ava/DATA3/DATA/Harika/inference/language/bert/data/dev-v1.1.json"
max_seq_length = 384
doc_stride = 128
max_query_length = 64

preprocessor = SquadPreprocessor()
input_ids, input_mask, segment_ids, eval_examples, eval_features = preprocessor.preprocess(
    input_file, max_seq_length, doc_stride, max_query_length)

print(f"Preprocessing complete.")
print(len(input_ids[0]))
import torch
import onnxruntime
import numpy as np
input_ids = np.array(input_ids[0], dtype=np.int64)
input_mask = np.array(input_mask[0], dtype=np.int64)
segment_ids = np.array(segment_ids[0], dtype=np.int64)
input_ids = np.expand_dims(input_ids, axis=0)
input_mask = np.expand_dims(input_mask , axis=0)
segment_ids = np.expand_dims(segment_ids, axis=0)
print(input_ids.shape)
print(input_mask.shape)
print(segment_ids.shape)
# exit()
# Convert to PyTorch tensors with batch size 1
# input_ids = torch.tensor(input_ids[0], dtype=torch.long).unsqueeze(0)
# input_mask = torch.tensor(input_mask[0], dtype=torch.long).unsqueeze(0)
# segment_ids = torch.tensor(segment_ids[0], dtype=torch.long).unsqueeze(0)
# print(input_ids)
# exit()

# Assuming model_input_ids, model_input_mask, model_segment_ids are the input names expected by your ONNX model
onnx_input = {
    "input_ids": input_ids,
    "input_mask": input_mask,
    "segment_ids": segment_ids
}

# Now you can send onnx_input to your ONNX model for inference
# For example, if using ONNX Runtime:
import onnxruntime as ort
model_path = "/media/ava/DATA3/DATA/Harika/inference/language/bert/build/data/bert_tf_v1_1_large_fp32_384_v2/model.onnx"
# Assuming model_path is the path to your ONNX model file
session = ort.InferenceSession(model_path)
output = session.run(None, onnx_input)
print(output)
print(output[0].shape)
print(output[1].shape)
exit()